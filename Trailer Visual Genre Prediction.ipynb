{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## So first before we go onto the model, let's look at the data\n",
    "Since our input X is going to be extremely large with almost 300 movie trailers, lets instead load in the average of each movie trailer and get a feel for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgtrailers = np.zeros((300,144,256,3))\n",
    "counter = 0\n",
    "for filename in os.listdir('MovieTrailers'):\n",
    "    cap = cv2.VideoCapture('MovieTrailers' + '/' + filename)\n",
    "    trailer = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            trailer.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    ## We add the np.uint8 to be able to visualize it with opencv\n",
    "    avgtrailer = np.around(np.mean(trailer,axis = 0)).astype(np.uint8)\n",
    "    # we add this line to force all averages to be the same size of 144x256\n",
    "    avgtrailer = cv2.resize(avgtrailer,(256,144)).astype(np.uint8)\n",
    "    cap.release()\n",
    "    avgtrailers[counter] = avgtrailer\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Learn a little bit about X, from the code segment below, we can see we have 300 values and each picture is rgb and 144x256 pixels in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples = 300\n",
      "Size of Picture  = (144, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Examples = \" + str(avgtrailers.shape[0]))\n",
    "print(\"Size of Picture  = \" + str(avgtrailers[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to load in the movie id, movie name, trailerurl, and the Genre (our label)\n",
    "Our Genres can be one of three categories : Action, Horror, Comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieId</th>\n",
       "      <th>MovieName</th>\n",
       "      <th>TrailerUrl</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Extraction</td>\n",
       "      <td>https://www.youtube.com/watch?v=L6P3nI6VnlY</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Gentleman</td>\n",
       "      <td>https://www.youtube.com/watch?v=Ify9S7hj480</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Code 8</td>\n",
       "      <td>https://www.youtube.com/watch?v=PrX1JJ5dduA</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>https://www.youtube.com/watch?v=TcMBFSGVi1c</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Star Wars: The Rise of Skywalker</td>\n",
       "      <td>https://www.youtube.com/watch?v=8Qn_spdM5Zg</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieId                         MovieName  \\\n",
       "0        1                        Extraction   \n",
       "1        2                     The Gentleman   \n",
       "2        3                            Code 8   \n",
       "3        4                 Avengers: Endgame   \n",
       "4        5  Star Wars: The Rise of Skywalker   \n",
       "\n",
       "                                    TrailerUrl   Genre  \n",
       "0  https://www.youtube.com/watch?v=L6P3nI6VnlY  Action  \n",
       "1  https://www.youtube.com/watch?v=Ify9S7hj480  Action  \n",
       "2  https://www.youtube.com/watch?v=PrX1JJ5dduA  Action  \n",
       "3  https://www.youtube.com/watch?v=TcMBFSGVi1c  Action  \n",
       "4  https://www.youtube.com/watch?v=8Qn_spdM5Zg  Action  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dataframe = pd.read_csv(\"MovieTrailerData.csv\")\n",
    "label_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make it a little easier to work with let's get the genre and turn it into a matrix of one-hot vectors where:\n",
    "- Action = [1,0,0]\n",
    "- Comedy = [0,1,0]\n",
    "- Horror = [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_dataframe[\"Genre\"].to_numpy()\n",
    "labels.reshape(labels.shape[0],1)\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(labels)\n",
    "labels = tf.keras.utils.to_categorical(vec)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Separate the Test/Dev/Test\n",
    "One thing we have to keep in mind is that we have a relatively small dataset, so what we ar egoing to do is do a 60/20/20 test/dev/test split. In addition we are going to make sure each of the sets get the respective amount of each of the categories. Then after we split them, we are going to shuffle them once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize split vars and sets\n",
    "train_percent =  .6\n",
    "dev_percent = .2\n",
    "test_percent = .2\n",
    "training_x = np.zeros((int(train_percent * avgtrailers.shape[0]),144,256,3))\n",
    "dev_x = np.zeros((int(dev_percent * avgtrailers.shape[0]),144,256,3))\n",
    "test_x = np.zeros((int(test_percent * avgtrailers.shape[0]),144,256,3))\n",
    "\n",
    "#Split the data by category first\n",
    "Action_Examples = avgtrailers[0:100]\n",
    "Horror_Examples = avgtrailers[100:200]\n",
    "Comedy_Examples = avgtrailers[200:300]\n",
    "\n",
    "#shuffle\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(Action_Examples)\n",
    "np.random.shuffle(Horror_Examples)\n",
    "np.random.shuffle(Comedy_Examples)\n",
    "\n",
    "#initialize splitting variables\n",
    "train_set_length = int(train_percent * Action_Examples.shape[0])\n",
    "dev_set_length = int(dev_percent * Action_Examples.shape[0])\n",
    "test_set_length = int(test_percent * Action_Examples.shape[0])\n",
    "train_set_end = train_set_length\n",
    "dev_set_end = training_set_end + dev_set_length\n",
    "test_set_end = dev_set_end + test_set_length\n",
    "\n",
    "#split the categories into the sets equally\n",
    "training_x[0:train_set_length] = Action_Examples[0:train_set_end]\n",
    "training_x[train_set_length :train_set_length * 2] = Horror_Examples[0:train_set_end]\n",
    "training_x[train_set_length*2:train_set_length * 3] = Comedy_Examples[0:train_set_end]\n",
    "\n",
    "dev_x[0:dev_set_length] = Action_Examples[train_set_end: dev_set_end]\n",
    "dev_x[dev_set_length :dev_set_length * 2] = Horror_Examples[train_set_end: dev_set_end]\n",
    "dev_x[dev_set_length*2:dev_set_length * 3] = Comedy_Examples[train_set_end: dev_set_end]\n",
    "\n",
    "test_x[0:test_set_length] = Action_Examples[dev_set_end: test_set_end]\n",
    "test_x[test_set_length:test_set_length * 2] = Horror_Examples[dev_set_end: test_set_end]\n",
    "test_x[test_set_length*2:test_set_length * 3] = Comedy_Examples[dev_set_end: test_set_end]\n",
    "\n",
    "#split the labels\n",
    "y_Action = labels[0:100]\n",
    "y_Horror = labels[100:200]\n",
    "y_Comedy = labels[200:300]\n",
    "\n",
    "training_y = np.zeros((int(train_percent * avgtrailers.shape[0]),3))\n",
    "dev_y = np.zeros((int(dev_percent * avgtrailers.shape[0]),3))\n",
    "test_y = np.zeros((int(test_percent * avgtrailers.shape[0]),3))\n",
    "\n",
    "training_y[0:train_set_length] = y_Action[0:train_set_end]\n",
    "training_y[train_set_length :train_set_length * 2] = y_Horror[0:train_set_end]\n",
    "training_y[train_set_length*2:train_set_length * 3] = y_Comedy[0:train_set_end]\n",
    "\n",
    "dev_y[0:dev_set_length] = y_Action[train_set_end: dev_set_end]\n",
    "dev_y[dev_set_length :dev_set_length * 2] = y_Horror[train_set_end: dev_set_end]\n",
    "dev_y[dev_set_length*2:dev_set_length * 3] = y_Comedy[train_set_end: dev_set_end]\n",
    "\n",
    "test_y[0:test_set_length] = y_Action[dev_set_end: test_set_end]\n",
    "test_y[test_set_length:test_set_length * 2] = y_Horror[dev_set_end: test_set_end]\n",
    "test_y[test_set_length*2:test_set_length * 3] = y_Comedy[dev_set_end: test_set_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 88.,  94.,  99.],\n",
       "        [ 88.,  95.,  99.],\n",
       "        [ 88.,  95.,  99.],\n",
       "        ...,\n",
       "        [ 84.,  88.,  92.],\n",
       "        [ 84.,  88.,  92.],\n",
       "        [ 83.,  87.,  91.]],\n",
       "\n",
       "       [[ 88.,  95.,  99.],\n",
       "        [ 88.,  95.,  99.],\n",
       "        [ 88.,  95.,  99.],\n",
       "        ...,\n",
       "        [ 85.,  89.,  93.],\n",
       "        [ 85.,  90.,  93.],\n",
       "        [ 84.,  88.,  91.]],\n",
       "\n",
       "       [[ 89.,  95., 100.],\n",
       "        [ 89.,  96., 100.],\n",
       "        [ 89.,  96., 100.],\n",
       "        ...,\n",
       "        [ 86.,  90.,  93.],\n",
       "        [ 86.,  90.,  93.],\n",
       "        [ 84.,  88.,  91.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 76.,  84.,  92.],\n",
       "        [ 76.,  84.,  91.],\n",
       "        [ 76.,  84.,  91.],\n",
       "        ...,\n",
       "        [ 83.,  91.,  96.],\n",
       "        [ 83.,  91.,  97.],\n",
       "        [ 83.,  91.,  96.]],\n",
       "\n",
       "       [[ 75.,  83.,  91.],\n",
       "        [ 75.,  83.,  91.],\n",
       "        [ 75.,  83.,  91.],\n",
       "        ...,\n",
       "        [ 82.,  90.,  96.],\n",
       "        [ 83.,  91.,  96.],\n",
       "        [ 82.,  90.,  95.]],\n",
       "\n",
       "       [[ 75.,  83.,  91.],\n",
       "        [ 75.,  83.,  91.],\n",
       "        [ 75.,  83.,  91.],\n",
       "        ...,\n",
       "        [ 82.,  90.,  95.],\n",
       "        [ 82.,  90.,  96.],\n",
       "        [ 82.,  90.,  95.]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgtrailers[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see a couple of datapoints\n",
    "Let's look at the first couple of datapoints, lets see the average of \"Avengers: Endgame\" (Action) , \"I See You\" (Horror), and \"Jumanji: The Next Level\" (Comedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('frame', avgtrailers[3])\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('NoteBookData/Avengers.jpg', avgtrailers[3])\n",
    "cv2.imshow('frame',avgtrailers[158])\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('NoteBookData/ISeeYou.jpg', avgtrailers[158])\n",
    "cv2.imshow('frame',avgtrailers[202])\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('NoteBookData/Jumanji.jpg', avgtrailers[202])\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avengers: Endgame\n",
    "![Avengers: Endgame](NoteBookData/Avengers.jpg)\n",
    "## I See You\n",
    "![I See You](NoteBookData/ISeeYou.jpg)\n",
    "## Jumanji: The Next Level\n",
    "![Jumanji: The Next Level](NoteBookData/Jumanji.jpg)\n",
    "\n",
    "So what do we notice, Well if you squint you can definitely see the rating for the movie I See You since the rating was kept on for a larger chunk of the trailer, we also see for the Jumanji average, we can see the ending title card. What can we do to remedy this? Do we even need to remedy this? Let's first try and build a simple logistic regression model to predict our genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Model (Without Pre-processing out Beginning and Ending Title/Rating/Producer Cards)\n",
    "## What will our model look like?\n",
    "Well since our pictures are 144x256x3, why don't we just make a logististic regression unit with 144x256x3 = 110,592 units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
